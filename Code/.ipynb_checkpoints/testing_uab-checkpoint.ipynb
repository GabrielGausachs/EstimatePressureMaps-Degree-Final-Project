{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c394c8c8-5d1d-4e09-925c-91795b8dd465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import crop\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import gc\n",
    "from torch.utils.data import DataLoader,Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48a3148-07ea-49ee-b079-464ddad6d9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ggausachs/DadesUAB/Data\n",
      "['231218151009', '231218171115', '231218184644', '231219111259', '231219121247', '231219125929', '231219160955', '231219163825', '231221152928', '231221160009', '231221173600', '231222090337', '240112122326', '240117120951', '240130104100', '240130110959', '240201120017', '240207155139', '240212155248', '240301122444', '240403123945']\n"
     ]
    }
   ],
   "source": [
    "def to_float32_and_scale(tensor,global_min,global_max):\n",
    "    tensor = tensor.float()\n",
    "    tensor = (tensor - global_min) / (global_max - global_min)\n",
    "    return tensor\n",
    "\n",
    "def crop_array(array):\n",
    "    \n",
    "    return crop(array, 20, 28, 85, 36)\n",
    "\n",
    "path_data = os.path.join(os.path.dirname(os.path.dirname((os.getcwd()))),'DadesUAB/Data')\n",
    "\n",
    "print(path_data)\n",
    "    \n",
    "transform = {\n",
    "            'input': transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(crop_array),\n",
    "                transforms.Resize((192, 84))]),\n",
    "            'output': transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.RandomHorizontalFlip(1),\n",
    "                transforms.Lambda(lambda x: to_float32_and_scale(x,0,905)),\n",
    "                transforms.Resize((192, 84))])\n",
    "}\n",
    "\n",
    "max_ir = -np.inf\n",
    "min_ir = np.inf\n",
    "max_pm = -np.inf\n",
    "min_pm = np.inf\n",
    "images_tensor = {}\n",
    "images_tensor['input']=[]\n",
    "images_tensor['output']=[]\n",
    "folders = os.listdir(path_data)\n",
    "folders_sorted = sorted(folders)\n",
    "print(folders_sorted)\n",
    "for folder in folders_sorted:\n",
    "    directory = os.path.join(path_data,folder)\n",
    "    #print(directory)\n",
    "    pattern = os.path.join(directory, '*IR.png')\n",
    "    files_ir = glob(pattern)\n",
    "    files_ir = sorted(files_ir)\n",
    "    #print(len(files_ir))\n",
    "    #print(files_ir)\n",
    "    pattern = os.path.join(directory, '*Pressio.csv')\n",
    "    files_pm = glob(pattern)\n",
    "    files_pm = sorted(files_pm)\n",
    "    #print(len(files_pm))\n",
    "    #print(files_pm)\n",
    "\n",
    "    for ir,pm in zip(files_ir,files_pm):\n",
    "        #print(ir)\n",
    "        #print(pm)\n",
    "        ir_array = mpimg.imread(ir)\n",
    "        array = np.rot90(ir_array, k=1, axes=(1,0))\n",
    "        array_2 = np.copy(array)\n",
    "        tensor_final = transform['input'](array_2)\n",
    "        images_tensor['input'].append(tensor_final)\n",
    "        ir_array_final = tensor_final.squeeze().numpy()\n",
    "        pm = pd.read_csv(pm)\n",
    "        pm_array = pm.to_numpy()\n",
    "        pm_array = np.rot90(pm_array, k=1, axes=(1,0))\n",
    "        pm_array_2 = np.copy(pm_array)\n",
    "        pm_tensor = transform['output'](pm_array_2)\n",
    "        images_tensor['output'].append(pm_tensor)\n",
    "        final_array = pm_tensor.squeeze().numpy()\n",
    "        \n",
    "        pmin, pmax = final_array.min(), final_array.max()\n",
    "        min_pm = min(min_pm, pmin)\n",
    "        max_pm = max(max_pm, pmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44a671b5-effd-4f18-8aa7-1419ceabfc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        self.inputs = data_dict['input']\n",
    "        self.outputs = data_dict['output']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_tensor = self.inputs[idx]\n",
    "        output_tensor = self.outputs[idx]\n",
    "        return input_tensor,output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1eafc61b-ece4-47c3-99d8-10f67a74b491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "UNET(\n",
      "  (ups): ModuleList(\n",
      "    (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): DoubleConv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (3): DoubleConv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (4): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (5): DoubleConv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (7): DoubleConv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (downs): ModuleList(\n",
      "    (0): DoubleConv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): DoubleConv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): DoubleConv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (3): DoubleConv(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bottleneck): DoubleConv(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Evaluation completed\n",
      "[MSELoss(), PerCS(), MSEeff(), SSIMMetric()]\n",
      "[0.003708047326654196, 0.5101315468316953, tensor(0.0103, device='cuda:0'), tensor(0.4602, device='cuda:0')]\n",
      "MSE - 0.003708047326654196, PerCS - 0.5101315468316953, MSEeff - 0.010299627669155598, SSIM - 0.46024441719055176\n"
     ]
    }
   ],
   "source": [
    "from Models import (\n",
    "    UNet,\n",
    "    UNet_phy\n",
    ")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_NAME = \"UNet\"\n",
    "name_model = \"Model Name\"\n",
    "\n",
    "models = {\"UNet\": UNet.UNET, \"UNet_phy\": UNet_phy.UNET_phy}\n",
    "\n",
    "features = [64,128,256,512]\n",
    "\n",
    "model = models[MODEL_NAME](1, 1,features).to(DEVICE)\n",
    "\n",
    "# Load the model\n",
    "model_file = os.path.join(os.path.join((os.getcwd()),'Models/SavedModels'),name_model)\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n",
    "    model.to(DEVICE)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"An error occurred while reading the file: {e}\")\n",
    "else:\n",
    "    print('Model Loaded')\n",
    "\n",
    "print(model)\n",
    "\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()  # Clean CUDA Cache if used GPU\n",
    "gc.collect()  # Collect trash to free memory not used\n",
    "\n",
    "\n",
    "custom_dataset = CustomDataset(images_tensor)\n",
    "\n",
    "test_loader = DataLoader(custom_dataset, batch_size=1, shuffle=False, num_workers=0, drop_last=True)\n",
    "\n",
    "total_metric = [0, 0, 0, 0]\n",
    "\n",
    "with torch.no_grad():\n",
    "        best_input_img = None\n",
    "        best_target_img = None\n",
    "        best_output_img = None\n",
    "        best_mse = np.inf\n",
    "\n",
    "        for batch_idx, (input_images, target_images) in enumerate(test_loader, 1):\n",
    "    \n",
    "                input_img = input_images.to(DEVICE)\n",
    "                target_img = target_images.to(DEVICE)\n",
    "    \n",
    "                output_img = model(input_img)\n",
    "    \n",
    "                for i, metric in enumerate(metrics):\n",
    "    \n",
    "                    test_metric = metric(output_img, target_img)\n",
    "    \n",
    "                    total_metric[i] += test_metric\n",
    "\n",
    "                    if i == 0:\n",
    "                        if best_mse > test_metric:\n",
    "                            best_input_img = input_img\n",
    "                            best_target_img = target_img\n",
    "                            best_output_img = output_img\n",
    "                            best_mse = test_metric\n",
    "    \n",
    "                # Free memory in each iteration\n",
    "                torch.cuda.empty_cache()  # Clean CUDA Cache if used GPU\n",
    "                gc.collect()  # Collect trash to free memory not used\n",
    "            \n",
    "print('Evaluation completed')\n",
    "epoch_metric = [(total_metric[0] /\n",
    "                    len(test_loader)).item(), total_metric[1] / len(test_loader),\n",
    "                    total_metric[2] / len(test_loader),total_metric[3] / len(test_loader)]\n",
    "    \n",
    "print(metrics)\n",
    "print(epoch_metric)\n",
    "    \n",
    "m_str = [f\"MSE - {epoch_metric[0]}\", f\"PerCS - {epoch_metric[1]}\", f\"MSEeff - {epoch_metric[2]}\", f\"SSIM - {epoch_metric[3]}\"]\n",
    "    \n",
    "m_str = \", \".join(m_str)\n",
    "    \n",
    "print(m_str)\n",
    "\n",
    "input_images = best_input_img.squeeze().cpu().numpy()\n",
    "target_images = best_target_img.squeeze().cpu().numpy()\n",
    "output_img = best_output_img.squeeze().cpu().numpy()\n",
    "\n",
    "plt.imshow(input_images)\n",
    "plt.axis('off')\n",
    "#plt.savefig(os.path.join(\"Input_Image.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Save the target image\n",
    "plt.imshow(target_images)\n",
    "plt.axis('off')\n",
    "#plt.savefig(os.path.join(\"Target_Image.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Save the output image\n",
    "plt.imshow(output_img)\n",
    "plt.axis('off')\n",
    "#plt.savefig(os.path.join(\"Output_Image.png\"))\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
