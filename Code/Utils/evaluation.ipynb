{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EVALUATION FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This file is to evaluate the loaded models with an specific validation keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import crop\n",
    "import gc\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from Utils.logger import initialize_logger, get_logger\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import signal\n",
    "from Utils.config import (\n",
    "    USE_PHYSICAL_DATA,\n",
    "    PATH_DATASET,\n",
    "    IMG_PATH,\n",
    "    MODEL_NAME,\n",
    ")\n",
    "\n",
    "from Utils import (\n",
    "    metrics,\n",
    ")\n",
    "# Metrics\n",
    "metrics = [\n",
    "    torch.nn.MSELoss(),\n",
    "    metrics.PerCS()\n",
    "]\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LOCAL_SLP_DATASET_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))),'SLP/danaLab')\n",
    "SERVER_SLP_DATASET_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))))),'mnt/DADES2/SLP/SLP/danaLab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the paths of the test arrays\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def read_json(f):\n",
    "    data = json.load(f)\n",
    "    if PATH_DATASET == 'Server':\n",
    "        return data\n",
    "    else:\n",
    "        dic_paths = {}\n",
    "\n",
    "        for key, paths in data.items():\n",
    "            dic_paths[key] = []\n",
    "            for i, path in enumerate(paths):\n",
    "                parts = path.split(\"danaLab\", 2) \n",
    "                result = \"SLP\".join(parts[1:]).lstrip('/')\n",
    "                modified_path = os.path.join(LOCAL_SLP_DATASET_PATH, result)\n",
    "                dic_paths[key].append(modified_path)\n",
    "\n",
    "        print(dic_paths)\n",
    "        \n",
    "        return dic_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get all the files of an especific cover or an especific patient\n",
    "\n",
    "covers = ['cover1','cover2','uncover']\n",
    "\n",
    "def filter_files(dic,filter):\n",
    "    filter_dic = {}\n",
    "    for key, paths in dic.items():\n",
    "        filter_dic[key]=[]\n",
    "        for path in paths:\n",
    "            if filter in path:\n",
    "                filter_dic[key].append(path)\n",
    "    return filter_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    # Dataset for the random option\n",
    "    # Includes:\n",
    "    # - IR arrays\n",
    "    # - PR arrays\n",
    "    # Physical data\n",
    "    def __init__(self, ir_paths, pm_paths, p_data, transform=None):\n",
    "\n",
    "        self.ir_paths = ir_paths\n",
    "        self.pm_paths = pm_paths\n",
    "        self.p_data = p_data\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ir_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        input_path = self.ir_paths[index]\n",
    "        output_path = self.pm_paths[index]\n",
    "\n",
    "        input_array = self.load_array(input_path)\n",
    "        output_array = self.load_array(output_path)\n",
    "        input_array = input_array.astype(np.float32)\n",
    "        output_array = output_array.astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            input_array = self.transform['input'](input_array)\n",
    "\n",
    "            if PATH_DATASET == 'Server':\n",
    "                parts = str(output_path.split(\"/\")[-4])\n",
    "            else:\n",
    "                parts = str(output_path.split(\"\\\\\")[-4])\n",
    "            number = int(parts)\n",
    "            p_vector = self.p_data.iloc[number-1]\n",
    "            weight = p_vector[1]\n",
    "            tensor_data = torch.tensor(p_vector.values)\n",
    "\n",
    "            # Applying median filter\n",
    "            median_array = signal.medfilt2d(output_array)\n",
    "            max_array = np.maximum(output_array, median_array)\n",
    "\n",
    "            area_m = 1.03226 / 10000\n",
    "            ideal_pressure = weight * 9.81 / (area_m * 1000)\n",
    "\n",
    "            output_array = (max_array / np.sum(max_array)) * ideal_pressure\n",
    "            output_array = self.transform['output'](output_array)\n",
    "\n",
    "            if USE_PHYSICAL_DATA:\n",
    "                return input_array, output_array, tensor_data\n",
    "            else:\n",
    "                return input_array, output_array\n",
    "\n",
    "    def load_array(self, path):\n",
    "        # Load the array\n",
    "        array = np.load(path)\n",
    "        return array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cover_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Function to get the files that we want and do the transforms and create a dataloader to pass the model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m pm_files, ir_files \u001b[38;5;241m=\u001b[39m cover_files(LOCAL_SLP_DATASET_PATH,patients,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcover1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m dic \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mir\u001b[39m\u001b[38;5;124m'\u001b[39m: ir_files, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpm\u001b[39m\u001b[38;5;124m'\u001b[39m: pm_files}\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrop_array\u001b[39m(array):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cover_files' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to get the files that we want and do the transforms and create a dataloader to pass the model\n",
    "\n",
    "def crop_array(array):\n",
    "        return crop(array,7, 29, 140, 66)\n",
    "\n",
    "\n",
    "def create_dataloader(dic,p_data,transform):\n",
    "    \n",
    "    dataset = CustomDataset(dic['ir'], dic['pm'], p_data, transform=transform)\n",
    "    print('Len dataset:',len(dataset))\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "            dataset, batch_size=1, shuffle=False, num_workers=0, drop_last=True)\n",
    "    \n",
    "    test_dataset_info = {\n",
    "        'Number of samples': len(test_loader.dataset),\n",
    "        'Batch size': test_loader.batch_size,\n",
    "        'Number of batches': len(test_loader)\n",
    "        }\n",
    "    \n",
    "    print(f\"Val loader info: {test_dataset_info}\")\n",
    "\n",
    "    return test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate with a model loaded the files that we want with the metrics\n",
    "\n",
    "def evaluation(model,model_file,metrics,test_loader):\n",
    "    model.load_state_dict(torch.load(model_file, map_location=torch.device('cpu')))\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    total_metric = [0, 0]\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_idx, (input_images, target_images) in enumerate(test_loader, 1):\n",
    "\n",
    "            input_img= input_images.to(DEVICE)\n",
    "            target_img = target_images.to(DEVICE)\n",
    "\n",
    "            output_img = model(input_img)\n",
    "\n",
    "            for i, metric in enumerate(metrics):\n",
    "\n",
    "                test_metric = metric(output_img, target_img)\n",
    "\n",
    "                total_metric[i] += test_metric\n",
    "\n",
    "            # Free memory in each iteration\n",
    "            del input_images\n",
    "            del target_images\n",
    "            torch.cuda.empty_cache()  # Clean CUDA Cache if used GPU\n",
    "            gc.collect()  # Collect trash to free memory not used\n",
    "\n",
    "    epoch_metric = [total_metric[0] /\n",
    "                    len(test_loader), total_metric[1] / len(test_loader)]\n",
    "    \n",
    "    print(metrics)\n",
    "    print(epoch_metric)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    axes[0].imshow(input_images.squeeze().cpu().numpy())\n",
    "    axes[0].set_title('Input Image')\n",
    "\n",
    "    axes[0].imshow(target_images.squeeze().cpu().numpy())\n",
    "    axes[0].set_title('Target Image')\n",
    "\n",
    "    axes[1].imshow(output_img.squeeze().cpu().numpy())\n",
    "    axes[1].set_title('Output Image')\n",
    "\n",
    "    \n",
    "\n",
    "    fig.suptitle('Input, Target and Output Image', fontsize=12)\n",
    "    fig.text(0.5, 0.01, f': {epoch_metric:.4f}', ha='center')\n",
    "    plt.savefig(os.path.join(IMG_PATH,'Comparing_output_model.png'))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execute this function to do an evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models import (\n",
    "    UNet,\n",
    "    Simple_net,\n",
    "    UNet_phy\n",
    ")\n",
    "\n",
    "# Models\n",
    "models = {\"Simple_net\": Simple_net.Simple_net,\n",
    "          \"UNet\": UNet.UNET, \"UNet_phy\": UNet_phy.UNET_phy}\n",
    "\n",
    "# Create a model\n",
    "if USE_PHYSICAL_DATA:\n",
    "        model = models[MODEL_NAME](1, 11, 1).to(DEVICE)\n",
    "else:\n",
    "        model = models[MODEL_NAME](1, 1).to(DEVICE)\n",
    "\n",
    "# Load the model\n",
    "model_file = os.path.join(os.path.dirname((os.getcwd())),'Models/statedict.pth')\n",
    "\n",
    "# Get the test paths\n",
    "f = open(os.path.join(os.path.dirname((os.getcwd())),'Models/TestJson/test_paths_20240506230502.json'))\n",
    "dic_paths =read_json(f)\n",
    "filter_dic = filter_files(dic_paths,'cover1')\n",
    "\n",
    "# Get the pdata\n",
    "if PATH_DATASET == 'Server':\n",
    "    path_arrays = SERVER_SLP_DATASET_PATH\n",
    "else:\n",
    "    path_arrays = LOCAL_SLP_DATASET_PATH\n",
    "p_data = pd.read_csv(os.path.join(path_arrays, 'physiqueData.csv'))\n",
    "p_data['gender'] = p_data['gender'].str.strip()\n",
    "p_data = pd.get_dummies(p_data, columns=['gender'])\n",
    "p_data = p_data.drop('sub_idx', axis=1)\n",
    "p_data['gender_male'] = p_data['gender_male'].astype(int)\n",
    "p_data['gender_female'] = p_data['gender_female'].astype(int)\n",
    "\n",
    "# Data transformation if needed\n",
    "transform = {\n",
    "        'input': transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Lambda(crop_array),  \n",
    "                    transforms.Resize((192, 84)),\n",
    "\t\t            transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "                    ]),\n",
    "        'output': transforms.Compose([transforms.ToTensor()])}\n",
    "\n",
    "# Create dataloader\n",
    "test_loader = create_dataloader(filter_dic,p_data,transform)\n",
    "\n",
    "# Do the evaluation\n",
    "evaluation(model,model_file,metrics,test_loader)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
