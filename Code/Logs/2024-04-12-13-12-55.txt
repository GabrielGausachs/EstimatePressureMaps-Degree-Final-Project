2024-04-12 13:12:56,963 - root - INFO -> Logger initialized in filename 2024-04-12-13-12-56
2024-04-12 13:12:56,964 - root - INFO -> --------------------------------------------------
2024-04-12 13:12:56,964 - root - INFO -> Reading the data from Server...
2024-04-12 13:12:57,049 - root - INFO -> Number of pacients: 102
2024-04-12 13:12:57,049 - root - INFO -> Number of categories in a patient: 3
2024-04-12 13:12:57,049 - root - INFO -> Calibration diccionary num of pacients: 102
2024-04-12 13:12:57,049 - root - INFO -> Number of values in a category for a patient: 45
2024-04-12 13:12:57,050 - root - INFO -> Shape of the IR numpy array: (160, 120)
2024-04-12 13:12:57,050 - root - INFO -> Max value of IR array: 30583
2024-04-12 13:12:57,050 - root - INFO -> Shape of the PM numpy array: (192, 84)
2024-04-12 13:12:57,050 - root - INFO -> Max value of PM array: 124.0
2024-04-12 13:12:57,050 - root - INFO -> --------------------------------------------------
2024-04-12 13:12:57,050 - root - INFO -> Creating dataset...
2024-04-12 13:12:57,050 - root - INFO -> Partition --> Patients
2024-04-12 13:12:57,051 - root - INFO -> Train size: 10935
2024-04-12 13:12:57,051 - root - INFO -> Val size: 2835
2024-04-12 13:12:57,051 - root - INFO -> --------------------------------------------------
2024-04-12 13:12:57,051 - root - INFO -> Creating dataloaders...
2024-04-12 13:12:57,051 - root - INFO -> Train loader info: {'Number of samples': 10935, 'Batch size': 128, 'Number of batches': 85}
2024-04-12 13:12:57,514 - root - INFO -> Array input size of the train loader: torch.Size([128, 1, 192, 84])
2024-04-12 13:12:57,676 - root - INFO -> Array output size of the train loader: torch.Size([128, 1, 192, 84])
2024-04-12 13:12:57,676 - root - INFO -> Val loader info: {'Number of samples': 2835, 'Batch size': 128, 'Number of batches': 22}
2024-04-12 13:12:57,799 - root - INFO -> Array input size of the val loader: torch.Size([128, 1, 192, 84])
2024-04-12 13:12:57,918 - root - INFO -> Array output size of the val loader: torch.Size([128, 1, 192, 84])
2024-04-12 13:13:17,968 - root - INFO -> --------------------------------------------------
2024-04-12 13:13:17,968 - root - INFO -> Wandb correctly initialized
2024-04-12 13:13:19,797 - root - INFO -> --------------------------------------------------
2024-04-12 13:13:19,798 - root - INFO -> Starting training with model UNet that has 31036481 parameters
2024-04-12 13:13:19,798 - root - INFO -> Learning rate: 0.0002
2024-04-12 13:13:19,798 - root - INFO -> Lambda value: 10
2024-04-12 13:13:19,798 - root - INFO -> --- Epoch: 0 ---
2024-04-12 13:13:19,798 - root - INFO -> Epoch: 0/5, Starting training...
2024-04-12 13:13:19,798 - root - INFO -> Loader length: 85
2024-04-12 13:13:19,798 - root - INFO -> Loader batch size: 128
2024-04-12 13:13:19,798 - root - INFO -> Loader drop last: True
2024-04-12 13:13:19,798 - root - INFO -> Loader num workers: 0
2024-04-12 13:13:19,798 - root - INFO -> Criterion: PWRSWtL()
2024-04-12 13:13:19,945 - root - INFO -> Memory cleaned!
2024-04-12 13:13:20,070 - root - INFO -> Epoch: 0/5, Processing batch 1/85...
2024-04-12 13:13:25,057 - root - INFO -> Epoch: 0/5, Processing batch 2/85...
2024-04-12 13:13:25,835 - root - INFO -> Epoch: 0/5, Processing batch 3/85...
2024-04-12 13:13:26,637 - root - INFO -> Epoch: 0/5, Processing batch 4/85...
2024-04-12 13:13:27,409 - root - INFO -> Epoch: 0/5, Processing batch 5/85...
2024-04-12 13:13:28,185 - root - INFO -> Epoch: 0/5, Processing batch 6/85...
2024-04-12 13:13:28,982 - root - INFO -> Epoch: 0/5, Processing batch 7/85...
2024-04-12 13:13:29,815 - root - INFO -> Epoch: 0/5, Processing batch 8/85...
2024-04-12 13:13:30,591 - root - INFO -> Epoch: 0/5, Processing batch 9/85...
2024-04-12 13:13:31,400 - root - INFO -> Epoch: 0/5, Processing batch 10/85...
2024-04-12 13:13:32,178 - root - INFO -> Epoch: 0/5, Processing batch 11/85...
2024-04-12 13:13:33,012 - root - INFO -> Epoch: 0/5, Processing batch 12/85...
2024-04-12 13:13:33,778 - root - INFO -> Epoch: 0/5, Processing batch 13/85...
2024-04-12 13:13:34,606 - root - INFO -> Epoch: 0/5, Processing batch 14/85...
2024-04-12 13:13:35,377 - root - INFO -> Epoch: 0/5, Processing batch 15/85...
2024-04-12 13:13:36,157 - root - INFO -> Epoch: 0/5, Processing batch 16/85...
2024-04-12 13:13:36,961 - root - INFO -> Epoch: 0/5, Processing batch 17/85...
